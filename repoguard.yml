# RepoGuard-AI configuration
# Enable or disable checks and optional features.
checks:
  sqli: true
  secrets: true
  xss: false
  prompt_injection: true


# Retrieval / RAG behavior: keep only likely-relevant snippets to keep LLM token usage low
rag_enabled: true


# LLM provider: openai, google, or local
provider: google

# LLM model to use (if using OpenAI or Google)
model: "gemini-2.5-flash-lite"

# Optional: set to your local LLM inference endpoint (takes priority over OpenAI key if set)
llm_endpoint: ""

# Maximum tokens to send per snippet (informational; the script uses heuristics)
max_snippet_tokens: 2000
